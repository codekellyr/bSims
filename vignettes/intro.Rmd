---
title: "Introduction"
author: "Peter Solymos"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
par(mar = c(1, 1, 1, 1))
```

## Introduction

The **bSims** R package is a _highly scientific_ and _utterly addictive_ bird point count simulator. Highly scientific, because it implements a spatially
explicit mechanistic simulation that is based on statistical models
widely used in bird point count analysis (i.e. removal models, distance 
sampling), and utterly addictive because the implementation
is designed to allow rapid exploration (via Shiny apps)
and efficient simulation (supporting various parallel backends),
thus elevating the user experience.

The goals of the package are to (1) allow easy testing of
statistical assumptions and explore effects of violating these assumptions,
to (2) aid survey design by comparing different options, and most
importantly, to (3) have fun while doing it.

## Simulation layers

Introductory stats books begin with the coin flip to introduce
the binomial distribution. In R we can easily simulate
an outcome from such a random variable
$Y \sim Binomial(1, p)$ doing something like this:

```{r sim-bin,eval=FALSE}
p <- 0.5

Y <- rbinom(1, size = 1, prob = p)
```

But a coin flip in reality is a lot more complicated: we might consider 
the initial force, the height of the toss, the spin, 
and the weight of the coin.

Bird behavior combined with the observation process presents
a more complicated system, that is often treated as a mixture of
a count distribution and a detection/nondetection process, e.g.:

```{r sim-pois,eval=FALSE}
D <- 2 # individuals / unit area
A <- 1 # area
p <- 0.8 # probability of availability given presence
q <- 0.5 # probability of detection given availability

N <- rpois(1, lambda = A * D)
Y <- rbinom(1, size = N, prob = p * q)
```

This looks not too complicated, corresponding to the true abundance
being a random variables $N \sim Poisson(DA)$, while the observed count
being $Y \sim Binomial(N, pq)$.
This is the exact simulation
that we need when we want to make sure that an _estimator_
is capable of estimating the _model_ parameters (`lambda` and `prob` here).
But such probabilistic simulations are not very useful when we are
interested how well the _model_ captures important aspects of _reality_.

Going back to the Poisson--Binomial example, `N` would be a result
of all the factors influencing bird abundance, such as
geographical location, season, habitat suitability, number of
conspecifics, competitors, or predators. `Y` however would
largely depend on how the birds behave depending on timing,
or how an observer might detect or miss the different individuals,
or count the same individual twice, etc.

Therefore the package has layers, that by default are 
_conditionally independent_ of each other. This design decision
is meant to facilitate the comparison of certain settings
while keeping all the underlying realizations identical, thus
helping to pinpoint effects without the extra variability introduced
by all the other effects.

The conditionally independent _layers_ of a 
**bSims** realization are the following, with the corresponding function:

1. landscape (`bsims_init`),
2. population (`bsims_populate`),
3. behavior with movement and vocalization events (`bsims_animate`),
4. the physical side of the observation process (`bsims_detect`), and
5. the human aspect of the observation process (`bsims_transcribe`).

See this example as a sneak peek that we'll explain in the following sections:

```{r intro,fig.height=6,fig.width=6}
library(bSims)

phi <- 0.5                 # singing rate
tau <- 1:3                 # EDR by strata
dur <- 10                  # simulation duration
tbr <- c(3, 5, 10)         # time intervals
rbr <- c(0.5, 1, 1.5, Inf) # counting radii

l <- bsims_init(10, 0.5, 1)# landscape
p <- bsims_populate(l, 1)  # population
e <- bsims_animate(p,      # events
  vocal_rate=phi, duration=dur)
d <- bsims_detect(e,       # detections
  tau=tau)
x <- bsims_transcribe(d,   # transcription
  tint=tbr, rint=rbr)

plot(x)
get_table(x)
```

### Landscape

The `bsims_ini` function sets up the geometry of a local landscape.
The `extent` of the landscape determines the edge lengths of a square shaped area.
With no argument values passed, the function assumes a homogeneous _habitat_ (H)
in a 10 units x 10 units landscape, 1 unit is 100 meters. Having units this way allows
easier conversion to ha as area unit that is often used in the North American bird literature.
As a result, our landscape has an area of 1 km$^2$.

The `road` argument defines the half-width of the road that is placed in a vertical position.
The `edge` argument defines the width of the edge stratum on both sides of the road.
Habitat (H), edge (E), and road (R) defines the 3 strata that we refer to by their initials (H for no stratification, HER for all 3 strata present).

The origin of the Cartesian coordinate system inside the landscape is centered at the middle of the square.
The `offset` argument allows the road and edge strata to be shifted to the left (negative values)
or to the right (positive values) of the horizontal axis. This makes it possible to create landscapes with only
two strata.
The `bsims_init` function returns a landscape object (with class 'bsims_landscape').


```{r landscape,fig.width=6, fig.height=7}
(l1 <- bsims_init(extent = 10, road = 0, edge = 0, offset = 0))
(l2 <- bsims_init(extent = 10, road = 1, edge = 0, offset = 0))
(l3 <- bsims_init(extent = 10, road = 0.5, edge = 1, offset = 2))
(l4 <- bsims_init(extent = 10, road = 0, edge = 5, offset = 5))

op <- par(mfrow = c(2, 2))
plot(l1, main = "Habitat")
points(0, 0, pch=3)
plot(l2, main = "Habitat & road")
lines(c(0, 0), c(-5, 5), lty=2)
plot(l3, main = "Habitat, edge, road + offset")
arrows(0, 0, 2, 0, 0.1, 20)
lines(c(2, 2), c(-5, 5), lty=2)
points(0, 0, pch=3)
plot(l4, main = "2 habitats")
arrows(0, 0, 5, 0, 0.1, 20)
lines(c(5, 5), c(-5, 5), lty=2)
points(0, 0, pch=3)
par(op)
```

### Population

The `bsims_populate` function _populates_ the landscape we created by the `bsims_init` function,
which is the first argument we have to pass to `bsims_populate`. The function returns a population
object (with class 'bsims_population').
The most important argument that controls how many individuals will inhabit our landscape is
`density` that defines the expected value of individuals per unit area (1 ha). By default,
`density = 1` ($D=1$) and we have 100 ha in the landscape ($A=100$) which 
translates into 100 individuals on average ($E[N]=\lambda=AD$).
The actual number of individuals in the landscape might deviate from this expectation,
because $N$ is a random variable ($N \sim f(\lambda)$). The `abund_fun` argument controls this relationship
between the expected ($\lambda$) and realized abundance ($N$). The default is a Poisson distribution:

```{r pop-pois}
bsims_populate(l1)
```

Changing `abund_fun` can be useful to make abundance constant or
allow under or overdispersion, e.g.:

```{r pop-nb}
summary(rpois(100, 100)) # Poisson variation
summary(MASS::rnegbin(100, 100, 0.8)) # NegBin variation
negbin <- function(lambda, ...) MASS::rnegbin(1, lambda, ...)
bsims_populate(l1, abund_fun = negbin, theta = 0.8)
## constant abundance
bsims_populate(l1, abund_fun = function(lambda, ...) lambda)
```

Once we determine how many individuals will populate the landscape, we have control over the
spatial arrangement of the nest location for each individual. The default is a homogeneous Poisson 
point process (complete spatial randomness). 
Deviations from this can be controlled by the `xy_fun`. This function takes
distance as its only argument and returns a numeric value between 0 and 1. A function
`function(d) reurn(1)` would be equivalent with the Poisson process, meaning that every new
random location is accepted with probability 1 irrespective of the distance between the new location and the 
previously generated point locations in the landscape.
When this function varies with distance, it leads to a non-homogeneous point process via this
accept-reject algorithm. The other arguments (`margin`, `maxit`, `fail`) are passed to the underlying
`accepreject` function to remove edge effects and handle high rejection rates.

In the next example, we fix the abundance to be constant (i.e. not a random variable, $N=\lambda$) 
and different spatial point processes:

```{r pop-xy,fig.height=9,fig.width=6}
D <- 0.5
f_abund <- function(lambda, ...) lambda

## systematic
f_syst <- function(d)
  (1-exp(-d^2/1^2) + dlnorm(d, 2)/dlnorm(exp(2-1),2)) / 2
## clustered
f_clust <- function(d)
  exp(-d^2/1^2) + 0.5*(1-exp(-d^2/4^2))

p1 <- bsims_populate(l1, density = D, abund_fun = f_abund)
p2 <- bsims_populate(l1, density = D, abund_fun = f_abund, xy_fun = f_syst)
p3 <- bsims_populate(l1, density = D, abund_fun = f_abund, xy_fun = f_clust)

distance <- seq(0,10,0.01)
op <- par(mfrow = c(3, 2))
plot(distance, rep(1, length(distance)), type="l", ylim = c(0, 1), 
  main = "random", ylab=expression(f(d)), col=2)
plot(p1)

plot(distance, f_syst(distance), type="l", ylim = c(0, 1), 
  main = "systematic", ylab=expression(f(d)), col=2)
plot(p2)

plot(distance, f_clust(distance), type="l", ylim = c(0, 1), 
  main = "clustered", ylab=expression(f(d)), col=2)
plot(p3)
par(op)
```

The `get_nests` function extracts the nest locations. `get_abundance`
and `get_density` gives the total abundance ($N$) and 
density ($D=N/A$, where $A$ is `extent^2`) in the lanscape, respectively.

If the landscape is stratified, that has no effect on density unless we specify different values
through the `density` argument as a vector of length 3 referring to the HER strata:

```{r pop-dens,fig.width=6, fig.height=7}
D <- c(H = 2, E = 0.5, R = 0)

op <- par(mfrow = c(2, 2))
plot(bsims_populate(l1, density = D), main = "Habitat")
plot(bsims_populate(l2, density = D), main = "Habitat & road")
plot(bsims_populate(l3, density = D), main = "Habitat, edge, road + offset")
plot(bsims_populate(l4, density = D), main = "2 habitats")
par(op)
```

### Behavior

The `bsims_animate` function _animates_ the population created by the `bsims_populate` function
that is the first of the function. `bsims_animate` returns an events objec (with class 'bsims_events').
The most important arguments are governing the `duration` of the simulation in minutes,
the vocalization (`vocal_rate`), and the movement (`move_rate`) rates as average number of events per minute.

We can describe these behavioral events using survival modeling terminology.
Event time ($T$) is a continuous random variable.
In the simplest case, its probability density function is the Exponential
distribution: $f(t)=\phi e^{-t\phi}$.
The corresponding cumulative distribution function is:
$F(t)=\int_{0}^{t} f(t)dt=1-e^{-t\phi}=p_t$,
giving the probability that the event has occurred by duration $t$. 
The parameter $\phi$ is the rate of the Exponential distribution
with mean $1/\phi$ and variance $1/\phi^2$.

In survival models, the complement of $F(t)$ is called the 
_survival function_ ($S(t)=1-F(t)$, $S(0)=1$),
which gives the probability that the event has not occurred by duration $t$.
The _hazard function_ ($\lambda(t)=f(t)/S(t)$) 
defines the instantaneous rate of occurrence of the event
(risk, the density of events at $t$ divided by the probability of surviving).
The cumulative hazard (cumulative risk) is the sum of the risks between duration 0 and $t$
($\Lambda(t)=\int_{0}^{t} \lambda(t)dt$).

The simplest survival distribution assumes constant risk over time ($\lambda(t)=\phi$),
which corresponds to the Exponential distribution.
The Exponential distribution also happens to describe the lengths of the 
inter-event times in a homogeneous Poisson process 
(events are independent, it is a 'memory-less' process).

`bsims_animate` uses independent Exponential distributions with 
rates `vocal_rate` and `move_rate` to simulate vocalization and movement events, respectively.
The `get_events` function extracts the events as a data frame
with columns describing the location (`x`, `y`) and time (`t`)
of the events (`v` is 1 for vocalizations and 0 otherwise) for 
each individual (`i` gives the individual identifier
that links individuals to the nest locations)

```{r beh-events}
l <- bsims_init()
p <- bsims_populate(l, density = 0.5)
e1 <- bsims_animate(p, vocal_rate = 1)

head(get_events(e1))
plot(get_events(e1))
curve((1-exp(-1*x)) * get_abundance(e1), col=2, add=TRUE)
```

There are no movement related events when `move_rate = 0`, the
individuals are always located at the nest, i.e. there is no
within territory movement.
If we increase the movement rate, we also have to increase
the value of `movement`, that is the standard deviation
of bivariate Normal kernels centered around each nest location.
This kernel is used to simulate new locations for the movement events.

```{r beh-move,fig.width=6, fig.height=3.5}
e2 <- bsims_animate(p, move_rate = 1, movement = 0.25)

op <- par(mfrow = c(1, 2))
plot(e1, main = "Closure")
plot(e2, main = "Movement")
par(op)
```

Individuals in the landscape might have different volcalization
rates depending on, e.g., berrding status. Such heterogeneity
can be added to the simulations as a finite mixture:
`vocal_rate` and `move_rate` can be supplied as a vector,
each element giving the rate for the groups.
The `mixture` argument is then used to specify the mixture
proportions.

```{r beh-mix,fig.width=6, fig.height=3.5}
e3 <- bsims_animate(p, 
  vocal_rate = c(25, 1), mixture = c(0.25, 0.75))

plot(get_events(e3))
curve((1-0.75*exp(-1*x)) * get_abundance(e3), col=2, add=TRUE)
```

Vocal and movement rates (and corrsponding kernel stadard deviations)
can be defined four different ways:

- a single number: constant behaviour patterns, no groups,
- a vector of length `length(mixture)`: behaviour based finite mixture groups,
- a vector of length 3 with `mixture = 1`: mixtures correspond to HER strata,
- or a matrix of dimension 3 $\times$ `length(mixture)`: HER strata $\times$ number of behaviour based groups.

Strata based groups are tracked by column `s`, 
behaviour based groups are tracked as the column `g` in
the output of `get_nests`.

Here is how different territory sizes can be achieved in
a two-habitat landscape:

```{r beh-move2,fig.width=3.5, fig.height=3.5}
plot(bsims_animate(bsims_populate(l4, density = D), 
  move_rate = c(0.5, 1, 1), movement = c(0, 0.2, 0.2), 
  mixture = 1), main="Strata based mixtures")
```

Stratum related behaviour groups depend on the nest location.
Sometimes it makes sense to restrict movement even further,
i.e. individuals do not land in certain strata
(but can cross a stratum if `movement` is large enough).
For example we can restrict movement into the road stratum
(this requires density to be 0 in that stratum):

```{r beh-move3,fig.width=6, fig.height=3.5}
op <- par(mfrow = c(1, 2))
plot(bsims_animate(bsims_populate(l2, density = D), 
  move_rate = 1, movement = 0.3, 
  avoid = "none"), main="Movement not restricted")
plot(bsims_animate(bsims_populate(l2, density = D), 
  move_rate = 1, movement = 0.3, 
  avoid = "R"), main="Movement restricted")
par(op)
```

Another way to restrict the movement of individuals is to
prevent the overlap based on a Voronoi tessellation
around the nest locations. Note: we are using the update method
here to update the `allow_overlap` argument of the 
previous call.

```{r beh-move,fig.width=6, fig.height=3.5}
e3 <- update(e2, allow_overlap=FALSE)

op <- par(mfrow = c(1, 2))
plot(e2, main = "Overlap")
plot(e3$tess, TRUE, "tess", "none", col="grey", lty=1)
plot(e3, main = "No overlap")
plot(e3$tess, TRUE, "tess", "none", col="grey", lty=1)
par(op)
```

We haven't mentioned the `initial_location` yet.
This allows to averride this whole layer and make all individuals
fully available for the other layers applied on top.
I.e. it is possible to study the observation process without
any behavioral interference when `initial_location = TRUE`.

### Detection

During the detection process, auditory and visual cues travel through
the distance between the bird and the observer.
As the power of the signal fades away, the
chanches of being detected also decreases.
This relationship is captured by the distance function ($g(d)$)
which describes the probability of detecting an individual
given the distance between the observer and the individual ($d$).
Distance functions are is a monotonic decreasing functions of distance,
and it is assumed thet detection at 0 distance is perfect ($g(0)=1$).

The most commonly used distance function is the Half-Normal. 
This is a one-parameter function ($g(d) = e^{-(d/\tau)^2}$) 
where probability initially remain high, reflecting an increased 
chance of detecting individuals closer to the observer
($\tau^2$ is variance of the unfolded Normal distribution, 
$\tau^2/2$ is the variance of the Half-Normal distribution).



```{r fig.show='hold',out.width='33%'}
d <- seq(0, 2, 0.01)
plot(d, exp(-d/0.8), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Negative Exponential")
plot(d, exp(-(d/0.8)^2), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Half-Normal")
plot(d, 1-exp(-(d/0.8)^-4), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Hazard rate")
```


```{block2, type='rmdexercise'}
**Exercise**

Try different values of $b$ to explore the different shapes of the Hazard rate function.

Write your own code (`plot(d, exp(-(d/<tau>)^<b>), type="l", ylim=c(0,1))`), or run `shiny::runApp("_shiny/distancefun.R")`.
```

We will apply this new found knowledge to our bSims world:
the observer is in the middle of the landscape, and each vocalization
event is aither detected or not, depending on the distance.
Units of `tau` are given on 100 m units, so that corresponding 
density estimates will refer to ha as the unit area.

In this example, we want all individuals to be equally available,
so we are going to override all behavioral aspects of the simulations
by the `initial_location` argument when calling `bsims_animate`.
We set `density` and `tau` high enough to detections in this example.

```{r}
tau <- 2

set.seed(123)
l <- bsims_init()
a <- bsims_populate(l, density=10)
b <- bsims_animate(a, initial_location=TRUE)

(o <- bsims_detect(b, tau=tau))
```

```{r fig.width=8,fig.height=8}
plot(o)
```



Args and what they do, cover support functions, use examples from Rd, cover theory briefly

```{r eval=FALSE}
bsims_detect(x, xy = c(0, 0), tau = 1, dist_fun = NULL,
  event_type = c("vocal", "move", "both"), ...)

# distance function exploration

# segmented model

tau <- c(1, 2, 3, 2, 1)
d <- seq(0, 4, 0.01)
dist_fun <- function(d, tau) exp(-(d/tau)^2) # half normal
#dist_fun <- function(d, tau) exp(-d/tau) # exponential
#dist_fun <- function(d, tau) 1-exp(-(d/tau)^-2) # hazard rate

b <- c(0.5, 1, 1.5, 2) #  boundaries

op <- par(mfrow=c(2, 1))
plot(d, dist_fun2(d, tau[1], dist_fun), type="n",
  ylab="P(detection)", xlab="Distance", axes=FALSE,
  main="Sound travels from left to right")
axis(1)
axis(2)
for (i in seq_len(length(b)+1)) {
  x1 <- c(0, b, 4)[i]
  x2 <- c(0, b, 4)[i+1]
  polygon(c(0, b, 4)[c(i, i, i+1, i+1)], c(0, 1, 1, 0),
    border=NA,
    col=c("darkolivegreen1", "burlywood1", "lightgrey",
    "burlywood1", "darkolivegreen1")[i])
}
lines(d, dist_fun2(d, tau[1], dist_fun))
lines(d, dist_fun2(d, tau[2], dist_fun))
lines(d, dist_fun2(d, tau[3], dist_fun))
lines(d, dist_fun2(d, tau, dist_fun, b), col=2, lwd=3)

plot(rev(d), dist_fun2(d, tau[1], dist_fun), type="n",
  ylab="P(detection)", xlab="Distance", axes=FALSE,
  main="Sound travels from right to left")
axis(1)
axis(2)
for (i in seq_len(length(b)+1)) {
  x1 <- c(0, b, 4)[i]
  x2 <- c(0, b, 4)[i+1]
  polygon(c(0, b, 4)[c(i, i, i+1, i+1)], c(0, 1, 1, 0),
    border=NA,
    col=c("darkolivegreen1", "burlywood1", "lightgrey",
    "burlywood1", "darkolivegreen1")[i])
}
lines(rev(d), dist_fun2(d, tau[1], dist_fun))
lines(rev(d), dist_fun2(d, tau[2], dist_fun))
lines(rev(d), dist_fun2(d, tau[3], dist_fun))
lines(rev(d), dist_fun2(d, tau, dist_fun, rev(4-b)), col=2, lwd=3)
par(op)
```

### Transcription

As part of the detection process, a skilled observer
counts individual birds at a count station.
New individuals are assigned to time and distance categories,
the type of behavior also registered.

Args and what they do, cover support functions, use examples from Rd, cover theory briefly


```{r eval=FALSE}
bsims_transcribe(x, tint = NULL, rint = Inf, error = 0,
  condition=c("event1", "det1", "alldet"),
  event_type=NULL, perception=NULL, ...)
```

## Simulation workflows

- Shiny apps: interactive exploration, copy/paste
- define corner cases
- use `expand_list` and `bsims_all`

## Single habitat case

```{r H,fig.height=10,fig.width=10}
phi <- 0.5
tau <- 2
Den <- 10

set.seed(1)
l <- bsims_init()
a <- bsims_populate(l, density=Den)
b <- bsims_animate(a, vocal_rate=phi)
o <- bsims_detect(b, tau=tau)

tint <- c(1, 2, 3, 4, 5)
rint <- c(0.5, 1, 1.5, 2) # truncated at 200 m
(tr <- bsims_transcribe(o, tint=tint, rint=rint))
(rem <- tr$removal) # binned new individuals
colSums(rem)
rowSums(rem)
plot(tr)
```

Estimating density with truncation in the single habitat case:

```{r Dtr}
library(detect)

## singing rate
fitp <- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type="rem")
phihat <- exp(fitp$coef)
c(true=phi, estimate=exp(fitp$coef))
(p <- 1-exp(-max(tint)*phihat))

## EDR
fitq <- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type="dis")
tauhat <- exp(fitq$coef)
c(true=tau, estimate=tauhat)
rmax <- max(rint)
(q <- (tauhat^2/rmax^2) * (1-exp(-(rmax/tauhat)^2)))

## density
(A <- pi * rmax^2)
Dhat <- sum(rem) / (A * p * q)
c(true=Den, estimate=Dhat)
```

Estimating density with unlimited distance in the single habitat case:

```{r Dinf}
rint <- c(0.5, 1, 1.5, 2, Inf) # unlimited

(tr <- bsims_transcribe(o, tint=tint, rint=rint))
(rem <- tr$removal) # binned new individuals
colSums(rem)
rowSums(rem)

fitp <- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type="rem")
phihat <- exp(fitp$coef)
c(true=phi, estimate=phihat)
(p <- 1-exp(-max(tint)*phihat))

fitq <- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type="dis")
tauhat <- exp(fitq$coef)
c(true=tau, estimate=tauhat)

(Ahat <- pi * tauhat^2)
q <- 1

Dhat <- sum(rem) / (Ahat * p * q)
c(true=Den, estimate=Dhat)
```

We have used so far a single location.
We also set the density unreasonably high to have enough 
counts for a reasonable estimate.
We can independently replicate the simulation for multiple
landscapes and analyze the results:

```{r Dx, eval=FALSE}
phi <- 0.5
tau <- 1
Den <- 1

tint <- c(3, 5, 10)
rint <- c(0.5, 1, 1.5, Inf)

sim_fun <- function() {
  l <- bsims_init()
  a <- bsims_populate(l, density=Den)
  b <- bsims_animate(a, vocal_rate=phi)
  o <- bsims_detect(b, tau=tau)
  bsims_transcribe(o, tint=tint, rint=rint)$rem
}

B <- 200
set.seed(123)
res <- pbapply::pbreplicate(B, sim_fun(), simplify=FALSE)

Ddur <- matrix(tint, B, length(tint), byrow=TRUE)
Ydur1 <- t(sapply(res, function(z) colSums(z)))
Ydur2 <- t(sapply(res, function(z) colSums(z[-nrow(z),])))
colSums(Ydur1) / sum(Ydur1)
colSums(Ydur2) / sum(Ydur2)
fitp1 <- cmulti(Ydur1 | Ddur ~ 1, type="rem")
fitp2 <- cmulti(Ydur2 | Ddur ~ 1, type="rem")
phihat1 <- unname(exp(coef(fitp1)))
phihat2 <- unname(exp(coef(fitp2)))

Ddis1 <- matrix(rint, B, length(rint), byrow=TRUE)
Ddis2 <- matrix(rint[-length(rint)], B, length(rint)-1, byrow=TRUE)
Ydis1 <- t(sapply(res, function(z) rowSums(z)))
Ydis2 <- t(sapply(res, function(z) rowSums(z)[-length(rint)]))
colSums(Ydis1) / sum(Ydis1)
colSums(Ydis2) / sum(Ydis2)
fitq1 <- cmulti(Ydis1 | Ddis1 ~ 1, type="dis")
fitq2 <- cmulti(Ydis2 | Ddis2 ~ 1, type="dis")
tauhat1 <- unname(exp(fitq1$coef))
tauhat2 <- unname(exp(fitq2$coef))

## unlimited correction
Apq1 <- pi * tauhat1^2 * (1-exp(-max(tint)*phihat1)) * 1
rmax <- max(rint[is.finite(rint)])
## truncated correction
Apq2 <- pi * rmax^2 * 
  (1-exp(-max(tint)*phihat2)) * 
  (tauhat2^2/rmax^2) * (1-exp(-(rmax/tauhat2)^2))

round(rbind(
  phi=c(true=phi, unlimited=phihat1, truncated=phihat2),
  tau=c(true=tau, unlimited=tauhat1, truncated=tauhat2),
  D=c(Den, unlimited=mean(rowSums(Ydis1))/Apq1,
      truncated=mean(rowSums(Ydis2))/Apq2)), 4)
```

## Shiny apps

Play with detection functions:

```{r shiny1,eval=FALSE}
run_app("distfunH")
run_app("distfunHER")
```

Play with simulations and explore biases in a single-habitat setting (see also https://psolymos.shinyapps.io/bSimsH/):

```{r shiny2,eval=FALSE}
run_app("bsimsH")
```

Play with simulations and explore biases in a stratified habitat setting with road and edge (see also https://psolymos.shinyapps.io/bSimsHER/):

```{r shiny3,eval=FALSE}
run_app("bsimsHER")
```

Shiny apps can be used to play with the settings, then
settings copied to the clipboard and eventually into R.




# The Detection Process -------------------------------------

## Introduction

As part of the detection process, a skilled observer
counts individual birds at a count station.
New individuals are assigned to time and distance categories,
the type of behavior also registered.
During this process, auditory cues travel through
the distance between the bird and the observer.
As the pover of the sound fades away, the
chanches of being detected also decreases.
If the detection process is based on visual detections,
vegetation can block line of sight, etc.
In this chapter, we scrutinize how this detection process
contributes to the factor $C$.

## Prerequisites

```{r det-libs,message=TRUE,warning=FALSE}
library(bSims)                # simulations
library(detect)               # multinomial models
library(Distance)             # distance sampling
load("_data/josm/josm.rda")   # JOSM data
source("functions.R")         # some useful stuff
```


## Distance functions

The distance function ($g(d)$ describes the probability of detecting an individual
given the distance between the observer and the individual ($d$).
The detection itself is often triggered by visual or auditory cues,
and thus depend on the individuals being available for detection 
(and of course being present in the survey area).

Distance functions have some characteristics:

- It is a monotonic decreasing function of distance,
- $g(0)=1$: detection at 0 distance is perfect.

Here are some common distance function and rationale for their use
(i.e. mechanisms leading to such distance shapes):

1. Negative Exponential: a one-parameter function ($g(d) = e^{-d/\tau}$), probability quickly decreases with distance, this mirrors sound attenuation under spherical spreading, so might be a suitable form for acoustic recoding devices (we will revisit this later), but not a very useful form for human based counts, as explained below;
2. Half-Normal: this is also a one-parameter function ($g(d) = e^{-(d/\tau)^2}$) where probability initially remain high (the _shoulder_), reflecting an increased chance of detecting individuals closer to the observer, this form has also sone practical advantages that we will discuss shortly ($\tau^2$ is variance of the unfolded Normal distribution, $\tau^2/2$ is the variance of the Half-Normal distribution -- both the Negative Exponential and the Half-Normal being special cases of $g(d) = e^{-(d/\tau)^b}$ that have the parameter $b$ [$b > 0$] affecting the shoulder);
3. Hazard rate: this is a two-parameter model ($g(d) = 1-e^{-(d/\tau)^-b}$) that have the parameter $b$ ($b > 0$) affecting the more pronounced and sharp shoulder.


```{r fig.show='hold',out.width='33%'}
d <- seq(0, 2, 0.01)
plot(d, exp(-d/0.8), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Negative Exponential")
plot(d, exp(-(d/0.8)^2), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Half-Normal")
plot(d, 1-exp(-(d/0.8)^-4), type="l", col=4, ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)", main="Hazard rate")
```


```{block2, type='rmdexercise'}
**Exercise**

Try different values of $b$ to explore the different shapes of the Hazard rate function.

Write your own code (`plot(d, exp(-(d/<tau>)^<b>), type="l", ylim=c(0,1))`), or run `shiny::runApp("_shiny/distancefun.R")`.
```

We will apply this new found knowledge to our bSims world:
the observer is in the middle of the landscape, and each vocalization
event is aither detected or not, depending on the distance.
Units of `tau` are given on 100 m units, so that corresponding 
density estimates will refer to ha as the unit area.

In this example, we want all individuals to be equally available,
so we are going to override all behavioral aspects of the simulations
by the `initial_location` argument when calling `bsims_animate`.
We set `density` and `tau` high enough to detections in this example.

```{r}
tau <- 2

set.seed(123)
l <- bsims_init()
a <- bsims_populate(l, density=10)
b <- bsims_animate(a, initial_location=TRUE)

(o <- bsims_detect(b, tau=tau))
```

```{r fig.width=8,fig.height=8}
plot(o)
```

## Distance sampling

The distribution of the _observed distances_ is a product of detectability
and the distribution of the individuals with respect to the point where
the observer is located.
For point counts, area increases linearly with radial distance, 
implying a triangular distribution with respect to the point
($h(d)=\pi 2 d /A=\pi 2 d / \pi r_{max}^2=2 d / r_{max}^2$, where 
$A$ is a circular survey area with truncation distance $r_{max}$).
The product $g(d) h(d)$ gives the density function of the observed distances.

```{r}
g <- function(d, tau, b=2, hazard=FALSE)
  if (hazard)
    1-exp(-(d/tau)^-b) else exp(-(d/tau)^b)
h <- function(d, rmax)
  2*d/rmax^2
```

```{r fig.show='hold',out.width='33%'}
rmax <- 4

d <- seq(0, rmax, 0.01)
plot(d, g(d, tau), type="l", col=4, ylim=c(0,1),
  xlab="d", ylab="g(d)", main="Prob. of detection")
plot(d, h(d, rmax), type="l", col=4,
  xlab="d", ylab="h(d)", main="PDF of distances")
plot(d, g(d, tau) * h(d, rmax), type="l", col=4,
  xlab="d", ylab="g(d) h(d)", main="Density of observed distances")
```

The object `da` contains the distances to all the nests
based on our bSims object,
we use this to display the distribution of available distances:

```{r}
da <- sqrt(rowSums(a$nests[,c("x", "y")]^2))

hist(da[da <= rmax], freq=FALSE, xlim=c(0, rmax),
  xlab="Available distances (d <= r_max)", main="")
curve(2*x/rmax^2, add=TRUE, col=2)
```

The `get_detections` function returns a data frame with the
detected events (in our case just the nest locations): 
`$d` is the distance, `$a` is the angle
(in degrees, counter clock-wise from positive x axis).

```{r}
head(dt <- get_detections(o))
```

The following code plots the probability density of the
observed distances within the truncation distance $r_{max}$,
thus we need to standardize the $g(r) h(r)$ function
by the integral sum:

```{r}
f <- function(d, tau, b=2, hazard=FALSE, rmax=1) 
  g(d, tau, b, hazard) * h(d, rmax)
tot <- integrate(f, lower=0, upper=rmax, tau=tau, rmax=rmax)$value

hist(dt$d[dt$d <= rmax], freq=FALSE, xlim=c(0, rmax),
  xlab="Observed distances (r <= rmax)", main="")
curve(f(x, tau=tau, rmax=rmax) / tot, add=TRUE, col=2)
```

In case of the Half-Normal, we can linearize the relationship
by taking the log of the distance function: 
$log(g(d)) =log(e^{-(d/\tau)^2})= -(d / \tau)^2 = x \frac{1}{\tau^2} = 0 + x \beta$.
Consequently, we can use GLM to fit a model with $x = -d^2$ as 
predictor and no intercept, and estimate $\hat{\beta}$ and
$\hat{\tau}=\sqrt{1/\hat{\beta}}$.

For this method to work, we need to know the observed and 
unobserved distances as well,
which makes this approach of low utility in practice when
location of unobserved individuals is unknown.
But we can at least check our bSims data:

```{r}
dat <- data.frame(
  distance=da, 
  x=-da^2, 
  detected=ifelse(rownames(o$nests) %in% dt$i, 1, 0))
summary(dat)
mod <- glm(detected ~ x - 1, data=dat, family=binomial(link="log"))
c(true=tau, estimate=sqrt(1/coef(mod)))
```

```{r}
curve(exp(-(x/sqrt(1/coef(mod)))^2), 
  xlim=c(0,max(dat$distance)), ylim=c(0,1),
  xlab="Distance (100 m)", ylab="P(detection)")
curve(exp(-(x/tau)^2), lty=2, add=TRUE)
rug(dat$distance[dat$detected == 0], side=1, col=4)
rug(dat$distance[dat$detected == 1], side=3, col=2)
legend("topright", bty="n", lty=c(2,1), 
  legend=c("True", "Estimated"))
```

The Distance package offers various tools to fit
models to observed distance data. 
See [here](https://workshops.distancesampling.org/duke-spatial-2015/practicals/1-detection-functions-solutions.html) for a tutorial.
The following script fits the Half-Normal (`key = "hn"`)
without ajustments (`adjustment=NULL`) to  
observed distance data from truncated point transect.
It estimates $\sigma = \sqrt{\tau}$:

```{r}
dd <- ds(dt$d, truncation = rmax, transect="point", 
  key = "hn", adjustment=NULL)
c(true=tau, estimate=exp(dd$ddf$par)^2)
```

## Average detection

To calculate the average probability of detecting individuals
within a circle with truncation distance $r_{max}$, we need to
integrate over the product of $g(r)$ and $h(r)$: 
$q(r_{max})=\int_{0}^{r_{max}} g(d) h(d) dd$.
This gives the volume of pie dough cut at $r_{max}$,
compared to the volume of the cookie cutter ($\pi r_{max}^2$).

```{r}
q <- sapply(d[d > 0], function(z)
  integrate(f, lower=0, upper=z, tau=tau, rmax=z)$value)

plot(d, c(1, q), type="l", col=4, ylim=c(0,1),
  xlab=expression(r[max]), ylab=expression(q(r[max])), 
  main="Average prob. of detection")
```

For the Half-Normal detection function, the analytical solution for the 
average probability is 
$\pi \tau^2 [1-exp(-d^2/\tau^2)] / (\pi r_{max}^2)$,
where the denominator is a normalizing constant
representing the volume of a cylinder of perfect detectability.

To visualize this, here is the pie analogy for 
$\tau=2$ and $r_{max}=2$:

```{r}
tau <- 2
rmax <- 2
w <- 0.1
m <- 2
plot(0, type="n", xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), 
  axes=FALSE, ann=FALSE)
yh <- g(rmax, tau=tau)
lines(seq(-rmax, rmax, rmax/100),
  g(abs(seq(-rmax, rmax, rmax/100)), tau=tau))
draw_ellipse(0, yh, rmax, w, lty=2)
lines(-c(rmax, rmax), c(0, yh))
lines(c(rmax, rmax), c(0, yh))
draw_ellipse(0, 0, rmax, w)
draw_ellipse(0, 1, rmax, w, border=4)
lines(-c(rmax, rmax), c(yh, 1), col=4)
lines(c(rmax, rmax), c(yh, 1), col=4)
```

## Binned distances

The cumulative density function for the Half-Normal
distribution ($\pi(r) = 1-e^{-(r/\tau)^2}$) is used to calculate
cell probabilities for binned distance data
(the normalizing constant is the area of the integral $\pi \tau^2$,
instead of $\pi r_{max}^2$).
It captures the proportion of the observed distances
relative to the whole volume of the observed distance density.
In the pie analogy, this is the dough volume inside
the cookie cutter, compared to the dough volume inside and outside
of the cutter (that happens to be $\pi \tau^2$ for the Half-Normal):

```{r}
plot(0, type="n", xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), 
  axes=FALSE, ann=FALSE)
yh <- g(rmax, tau=tau)
lines(seq(-m*rmax, m*rmax, rmax/(m*100)),
  g(seq(-m*rmax, m*rmax, rmax/(m*100)), tau=tau),
  col=2)
lines(seq(-rmax, rmax, rmax/100),
  g(abs(seq(-rmax, rmax, rmax/100)), tau=tau))
draw_ellipse(0, yh, rmax, w, lty=2)
lines(-c(rmax, rmax), c(0, yh))
lines(c(rmax, rmax), c(0, yh))
draw_ellipse(0, 0, rmax, w)
```

In case of the Half-Normal distance function,
$\tau$ is the _effective detection radius_ (EDR). 
The effective detection radius is the distance from observer 
where the number of individuals missed within EDR 
(volume of 'air' in the cookie cutter above the dough)
equals the number of individuals detected outside of EDR
(dough volume outside the cookie cutter),
EDR is the radius $r_e$ where $q(r_e)=\pi(r_e)$:

```{r}
plot(0, type="n", xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), 
  axes=FALSE, ann=FALSE)
yh <- g(rmax, tau=tau)
lines(seq(-m*rmax, m*rmax, rmax/(m*100)),
  g(seq(-m*rmax, m*rmax, rmax/(m*100)), tau=tau),
  col=2)
lines(seq(-rmax, rmax, rmax/100),
  g(abs(seq(-rmax, rmax, rmax/100)), tau=tau))
draw_ellipse(0, yh, rmax, w, lty=2)
lines(-c(rmax, rmax), c(0, yh))
lines(c(rmax, rmax), c(0, yh))
draw_ellipse(0, 0, rmax, w)
draw_ellipse(0, 1, rmax, w, border=4)
lines(-c(rmax, rmax), c(yh, 1), col=4)
lines(c(rmax, rmax), c(yh, 1), col=4)

```

```{block2, type='rmdexercise'}
**Exercise**

What would be a computational algorithm to calculate EDR
for any distance function and truncation distance?

Trye to explain how the code below is working.

Why are EDRs different for different truncation distances?
```

```{r}
find_edr <- function(dist_fun, ..., rmax=Inf) {
  ## integral function
  f <- function(d, ...)
    dist_fun(d, ...) * 2*d*pi
  ## volume under dist_fun
  V <- integrate(f, lower=0, upper=rmax, ...)$value
  u <- function(edr)
    V - edr^2*pi
  uniroot(u, c(0, 1000))$root
}

find_edr(g, tau=1)
find_edr(g, tau=10)
find_edr(g, tau=1, b=1)
find_edr(g, tau=1, b=4, hazard=TRUE)

find_edr(g, tau=1, rmax=1)
```

The function $\pi(r)$ increases monotonically from 0 to 1:

```{r}
curve(1-exp(-(x/tau)^2), xlim=c(0, 5), ylim=c(0,1), col=4,
  ylab=expression(pi(d)), xlab=expression(d), 
  main="Cumulative density")
```

Here are binned distances for the bSims data, with expected
proportions based on $\pi()$ cell probabilities 
(differences within the distance bins).
The nice thing about this cumulative density formulation
is that it applies equally to truncated and unlimited
(not truncated) distance data, and the radius end point
for a bin (stored in `br`) can be infinite:

```{r}
br <- c(1, 2, 3, 4, 5, Inf)
dat$bin <- cut(da, c(0, br), include.lowest = TRUE)
(counts <- with(dat, table(bin, detected)))

pi_br <- 1-exp(-(br/tau)^2)

barplot(counts[,"1"]/sum(counts[,"1"]), space=0, col=NA,
  xlab="Distance bins (100 m)", ylab="Proportions",
  ylim=c(0, max(diff(c(0, pi_br)))))
lines(seq_len(length(br))-0.5, diff(c(0, pi_br)), col=3)
```

We can use the `bsims_transcribe` function for the same effect,
and estimate $\hat{\tau}$ based on the binned data:

```{r}
(tr <- bsims_transcribe(o, rint=br))
tr$removal

Y <- matrix(drop(tr$removal), nrow=1)
D <- matrix(br, nrow=1)

tauhat <- exp(cmulti.fit(Y, D, type="dis")$coef)

c(true=tau, estimate=tauhat)
```

Here are cumulative counts and the true and expected
cumulative cell probabilities:

```{r}
plot(stepfun(1:6, c(0, cumsum(counts[,"1"])/sum(counts[,"1"]))), 
  do.points=FALSE, main="Binned CDF",
  ylab="Cumulative probability", 
  xlab="Bin radius end point (100 m)")
curve(1-exp(-(x/tau)^2), col=2, add=TRUE)
curve(1-exp(-(x/tauhat)^2), col=4, add=TRUE)
legend("topleft", bty="n", lty=1, col=c(2, 4, 1), 
  legend=c("True", "Estimated", "Empirical"))
```

## Availability bias

We have ignored availability so far when working with bSims, 
but can't continue like that for real data.
What this means, is that $g(0) < 1$, so detecting
an individual 0 distance from the observer depends on
an event (visual or auditory) that would trigger the detection.
For example, if a perfecly camouflaged birds sits in silence,
detection might be difficult. Movement, or a vocalization
can, however, reveal the individual and its location.

The `phi` and `tau` values are at the high end of plausible
values for songbirds. The `Den`sity value is exaggerated,
but this way we will have enough counts to prove our points
using bSims:

```{r}
phi <- 0.5
tau <- 2
Den <- 10
```

Now we go through the layers of our bSims world:

1. initiating the landscape,
2. populating the landscape by individuals,
3. breath life into the virtual birds and let them sing,
4. put in an observer and let the observation process begin.

```{r}
set.seed(2)
l <- bsims_init()
a <- bsims_populate(l, density=Den)
b <- bsims_animate(a, vocal_rate=phi)
o <- bsims_detect(b, tau=tau)
```

Transcription is the process of turning the detections
into a table showing new individuals detected by
time intervals and distance bands, as defined
by the `tint` and `rint` arguments, respectively.

```{r}
tint <- c(1, 2, 3, 4, 5)
rint <- c(0.5, 1, 1.5, 2) # truncated at 200 m
(tr <- bsims_transcribe(o, tint=tint, rint=rint))
(rem <- tr$removal) # binned new individuals
colSums(rem)
rowSums(rem)
```

The plot method displays the detections presented as part of the `tr` object.

```{r fig.width=8,fig.height=8}
plot(tr)
```

The detection process and the transcription (following a prescribed
protocol) is inseparable in the field. However, recordings
made in the field can be processed by a number of different ways.
Separating these processed gives the ability to make these
conparisons on the exact same set of detections.

## Estimating density with truncation

We now fit the removal model to the data pooled by time intervals.
`p` is the cumulative probability of availability for the total duration:

```{r}
fitp <- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type="rem")
phihat <- exp(fitp$coef)
c(true=phi, estimate=exp(fitp$coef))
(p <- 1-exp(-max(tint)*phihat))
```

The distance sampling model uses the distance binned counts,
and a Half-Normal detection function, `q` is the cumulative
probability of perceptibility within the area of 
truncation distance `rmax`:

```{r}
fitq <- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type="dis")
tauhat <- exp(fitq$coef)
c(true=tau, estimate=tauhat)
rmax <- max(rint)
(q <- (tauhat^2/rmax^2) * (1-exp(-(rmax/tauhat)^2)))
```

The known `A`rea, `p`, and `q` makes up the correction factor,
which is used to estimate density based on $\hat{D}=Y/(A \hat{p}\hat{q})$:

```{r}
(A <- pi * rmax^2)
Dhat <- sum(rem) / (A * p * q)
c(true=Den, estimate=Dhat)
```

## Unlimited distance

We now change the distance bins to include the area outside of the
previous `rmax` distance, making the counts unlimited distance counts:

```{r}
rint <- c(0.5, 1, 1.5, 2, Inf) # unlimited

(tr <- bsims_transcribe(o, tint=tint, rint=rint))
(rem <- tr$removal) # binned new individuals
colSums(rem)
rowSums(rem)
```

The removal model is basically the same, the only
difference is that the counts can be higher due to
detecting over larger area and thus potentially
detecting more individuals:

```{r}
fitp <- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type="rem")
phihat <- exp(fitp$coef)
c(true=phi, estimate=phihat)
(p <- 1-exp(-max(tint)*phihat))
```

The diatance sampling model also takes the extended data set.

```{r}
fitq <- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type="dis")
tauhat <- exp(fitq$coef)
c(true=tau, estimate=tauhat)
```

The problem is that our truncation distance is infinite,
thus the area that we are sampling is also infinite.
This does not make too much sense, and not at all hepful
in estimating density (anything divided by infinity is 0).
So we use EDR (`tauhat` for Half-Normal) and calculate
the estimated effective area sampled (`Ahat`; $\hat{A}=\pi \hat{\tau}^2$).
We also set `q` to be 1, because the logic behind EDR is that its
volume equals the volume of the integral, in other words,
it is an area that would give on average same count under perfect detection
Finally, we estimate density using $\hat{D}=Y/(\hat{A} \hat{p}1)$

```{r}
(Ahat <- pi * tauhat^2)
q <- 1

Dhat <- sum(rem) / (Ahat * p * q)
c(true=Den, estimate=Dhat)
```

## Replicating landscapes

Remember, that we have used so far a single location.
We set the density unreasonably high to have enough counts
for a reasonable estimate.
We can independently replicate the simulation for multiple
landscapes and analyze the results to give justice to bSims
under idealized conditions:

```{r eval=FALSE}
phi <- 0.5
tau <- 1
Den <- 1

tint <- c(3, 5, 10)
rint <- c(0.5, 1, 1.5, Inf)

sim_fun <- function() {
  l <- bsims_init()
  a <- bsims_populate(l, density=Den)
  b <- bsims_animate(a, vocal_rate=phi)
  o <- bsims_detect(b, tau=tau)
  bsims_transcribe(o, tint=tint, rint=rint)$rem
}

B <- 200
set.seed(123)
res <- pbapply::pbreplicate(B, sim_fun(), simplify=FALSE)

Ddur <- matrix(tint, B, length(tint), byrow=TRUE)
Ydur1 <- t(sapply(res, function(z) colSums(z)))
Ydur2 <- t(sapply(res, function(z) colSums(z[-nrow(z),])))
colSums(Ydur1) / sum(Ydur1)
colSums(Ydur2) / sum(Ydur2)
fitp1 <- cmulti(Ydur1 | Ddur ~ 1, type="rem")
fitp2 <- cmulti(Ydur2 | Ddur ~ 1, type="rem")
phihat1 <- unname(exp(coef(fitp1)))
phihat2 <- unname(exp(coef(fitp2)))

Ddis1 <- matrix(rint, B, length(rint), byrow=TRUE)
Ddis2 <- matrix(rint[-length(rint)], B, length(rint)-1, byrow=TRUE)
Ydis1 <- t(sapply(res, function(z) rowSums(z)))
Ydis2 <- t(sapply(res, function(z) rowSums(z)[-length(rint)]))
colSums(Ydis1) / sum(Ydis1)
colSums(Ydis2) / sum(Ydis2)
fitq1 <- cmulti(Ydis1 | Ddis1 ~ 1, type="dis")
fitq2 <- cmulti(Ydis2 | Ddis2 ~ 1, type="dis")
tauhat1 <- unname(exp(fitq1$coef))
tauhat2 <- unname(exp(fitq2$coef))

## unlimited correction
Apq1 <- pi * tauhat1^2 * (1-exp(-max(tint)*phihat1)) * 1
rmax <- max(rint[is.finite(rint)])
## truncated correction
Apq2 <- pi * rmax^2 * 
  (1-exp(-max(tint)*phihat2)) * 
  (tauhat2^2/rmax^2) * (1-exp(-(rmax/tauhat2)^2))

round(rbind(
  phi=c(true=phi, unlimited=phihat1, truncated=phihat2),
  tau=c(true=tau, unlimited=tauhat1, truncated=tauhat2),
  D=c(Den, unlimited=mean(rowSums(Ydis1))/Apq1,
      truncated=mean(rowSums(Ydis2))/Apq2)), 4)
##     true unlimited truncated
## phi  0.5    0.4835    0.4852
## tau  1.0    1.0002    0.9681
## D    1.0    1.0601    1.1048
```

```{block2, type='rmdexercise'}
**Exercise**

If time permits, try different settings and time/distance intervals.
```

## JOSM data

Quickly organize the JOSM data:

```{r}
## predictors
x <- josm$surveys
x$FOR <- x$Decid + x$Conif+ x$ConifWet # forest
x$AHF <- x$Agr + x$UrbInd + x$Roads # 'alienating' human footprint
x$WET <- x$OpenWet + x$ConifWet + x$Water # wet + water
cn <- c("Open", "Water", "Agr", "UrbInd", "SoftLin", "Roads", "Decid", 
  "OpenWet", "Conif", "ConifWet")
x$HAB <- droplevels(find_max(x[,cn])$index) # drop empty levels
levels(x$HAB)[levels(x$HAB) %in% 
  c("OpenWet", "Water", "Open", "Agr", "UrbInd", "Roads")] <- "Open"
levels(x$HAB)[levels(x$HAB) %in% 
  c("Conif", "ConifWet")] <- "Conif"
x$OBS <- as.factor(x$ObserverID)

## time intervals
yall_dur <- Xtab(~ SiteID + Dur + SpeciesID, 
  josm$counts[josm$counts$DetectType1 != "V",])
yall_dur <- yall_dur[sapply(yall_dur, function(z) sum(rowSums(z) > 0)) > 100]

## distance intervals
yall_dis <- Xtab(~ SiteID + Dis + SpeciesID, 
  josm$counts[josm$counts$DetectType1 != "V",])
yall_dis <- yall_dis[sapply(yall_dis, function(z) sum(rowSums(z) > 0)) > 100]
```

Pick our most abundant species again, and organize the data:

```{r}
spp <- "TEWA"

Ydur <- as.matrix(yall_dur[[spp]])
Ddur <- matrix(c(3, 5, 10), nrow(Ydur), 3, byrow=TRUE,
  dimnames=dimnames(Ydur))
stopifnot(all(rownames(x) == rownames(Ydur)))

Ydis <- as.matrix(yall_dis[[spp]])
Ddis <- matrix(c(0.5, 1, Inf), nrow(Ydis), 3, byrow=TRUE,
  dimnames=dimnames(Ydis))
stopifnot(all(rownames(x) == rownames(Ydis)))

colSums(Ydur)
colSums(Ydis)
```

We pick a removal models with `DAY` as covariate,
and calculate $p(t)$:

```{r}
Mdur <- cmulti(Ydur | Ddur ~ DAY, x, type="rem")
summary(Mdur)
phi <- exp(model.matrix(Mdur) %*% coef(Mdur))
summary(phi)
p <- 1-exp(-10*phi)
```

We fit the intercept only distance sampling model next:

```{r}
Mdis0 <- cmulti(Ydis | Ddis ~ 1, x, type="dis")
summary(Mdis0)
```

Let's try a few covariates:

- continuous `FOR`est cover covariate: sound attenuation increases with forest cover;
- discrete `HAB`itat has 3 levels: open, deciduous forest, and coniferous forest (based on dominant land cover), because broad leaves and needles affect sound attenuation;
- finally, we use observer ID as categorical variable: observers might have different hearing abilities, training/experiance levels, good times, bad times, etc.

```{r}
Mdis1 <- cmulti(Ydis | Ddis ~ FOR, x, type="dis")
Mdis2 <- cmulti(Ydis | Ddis ~ HAB, x, type="dis")
```

We can look at AIC to find the best supported model:

```{r}
aic <- AIC(Mdis0, Mdis1, Mdis2)
aic$delta_AIC <- aic$AIC - min(aic$AIC)
aic[order(aic$AIC),]

Mdis <- get(rownames(aic)[aic$delta_AIC == 0])
summary(Mdis)
```

```{block2, type='rmdexercise'}
**Exercise**

Use `OBS` as predictor for `tau` and look at predicted EDRs.

What is the practical issue with using observer as predictor?
```

After finding the best model, we predict `tau`:

```{r}
tau <- exp(model.matrix(Mdis) %*% coef(Mdis))
boxplot(tau ~ HAB, x)
```

Finally, we calculate the correction factor for unlimited distances,
and predict mean density:

```{r}
Apq <- pi * tau^2 * p * 1
x$ytot <- rowSums(Ydur)
mean(x$ytot / Apq)
```

Alternatively, we can use the log of the correction
as an offset in log-linear models. This offset is 
called the QPAD offset:

```{r}
off <- log(Apq)
m <- glm(ytot ~ 1, data=x, offset=off, family=poisson)
exp(coef(m))
```

```{block2, type='rmdexercise'}
**Exercise**

Try distance sampling and density estimation for another species.

Fit multiple GLMs with QPAD offsets and covariates affecting density,
interpret the results and the visualize responses.
```

Sometimes, a recording is made at the survey location
that is listened to and transcribed in the lab
using headphones and possibly a computer screen.
This presents new challenges, and also new opportunities
for analysis of count data -- and is the topic of the next chapter.


